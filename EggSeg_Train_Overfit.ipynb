{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1mt9NeD5kOk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 0) Notebook outline\n",
        "\n",
        "1. Environment check & installs\n",
        "2. Config (paths + hyperparams)\n",
        "3. Quick data sanity (shape, spacing, histogram)\n",
        "4. Preprocessing (reorient → resample → normalize)\n",
        "5. Class weights (from the mask)\n",
        "6. Dataset & patch sampler (class-balanced)\n",
        "7. Model / loss / optimizer\n",
        "8. Train loop + lightweight logging\n",
        "9. Full-volume inference (sliding window)\n",
        "10. Post-processing (largest CC per class)\n",
        "11. Metrics (per-class Dice)\n",
        "12. Visualization (overlay slices)\n",
        "13. Export artifacts (ckpt, pred, config snapshot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YTCMxVb5kOl"
      },
      "source": [
        "## 1) Environment check & installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPtyLRm55qTS",
        "outputId": "1107e617-b3d0-4445-9669-cc8575083ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: data/egg_label.nii (deflated 97%)\n",
            "  adding: data/mri_single_egg.nii (deflated 6%)\n",
            "  adding: data/outputs/ (stored 0%)\n",
            "  adding: data/preprocessed/ (stored 0%)\n"
          ]
        }
      ],
      "source": [
        "# !unzip data.zip\n",
        "!zip data.zip data/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ifw_PZel6VQQ"
      },
      "outputs": [],
      "source": [
        "# !pip install -q numpy scipy scikit-image scikit-learn pandas seaborn plotly matplotlib  notebook  nibabel SimpleITK pytest tqdm pyyaml monai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aTdCMQaM5kOl"
      },
      "outputs": [],
      "source": [
        "import os, json, math, random, time, numpy as np\n",
        "import torch, nibabel as nib\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# MONAI transforms (data preprocessing and augmentation for medical images)\n",
        "import monai\n",
        "\n",
        "from monai.transforms import (\n",
        "    Compose,                        # Compose multiple transforms together\n",
        "    LoadImaged,                     # Load image and metadata dictionary\n",
        "    EnsureChannelFirstd,            # Ensure image tensor has channels as first dimension\n",
        "    Orientationd,                   # Reorient image to a canonical orientation\n",
        "    Spacingd,                       # Resample images to given voxel spacing\n",
        "    ScaleIntensityRangePercentilesd,# Intensity scaling based on percentile range\n",
        "    NormalizeIntensityd,            # Normalize image intensities\n",
        "    ToTensord,                      # Convert numpy arrays to PyTorch tensors\n",
        "    RandFlipd,                      # Randomly flip images along spatial axes\n",
        "    RandRotate90d,                  # Randomly rotate images by 90 degrees\n",
        "    RandAffined,                    # Random affine transformations (rotate, scale, shear, etc.)\n",
        "    RandGaussianNoised,             # Add random Gaussian noise\n",
        "    RandAdjustContrastd,            # Randomly adjust image contrast\n",
        "    RandCropByLabelClassesd,        # Random crop patches around label classes\n",
        "    AsDiscreted,                    # Convert predictions to discrete labels\n",
        "    KeepLargestConnectedComponentd, # Keep only the largest connected region (e.g., tumor mask)\n",
        "    SaveImaged,                     # Save transformed images to disk\n",
        "    DivisiblePadd,                  # Pad image size to be divisible by a given factor\n",
        "    EnsureTyped                     # Ensure data types are consistent (torch.Tensor, MetaTensor, etc.)\n",
        ")\n",
        "\n",
        "# MONAI data structures and utilities\n",
        "from monai.data.meta_tensor import MetaTensor        # Tensor with metadata support\n",
        "from monai.data import Dataset, DataLoader, NibabelWriter  # Dataset/DataLoader classes, NIfTI writer\n",
        "\n",
        "# MONAI MODELS\n",
        "from monai.networks.nets import UNet  # U-Net architecture for medical image segmentation\n",
        "\n",
        "# MONAI losses\n",
        "from monai.losses import DiceCELoss, DiceLoss  # Loss functions for segmentation tasks\n",
        "\n",
        "# MONAI inference utilities\n",
        "from monai.inferers import sliding_window_inference  # Inference method for large 3D images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM-4bIhKNzAu",
        "outputId": "0773561a-2d55-478b-8f64-3eddf331554a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.6.0+cu124 CUDA: False MONAI: 1.5.0\n"
          ]
        }
      ],
      "source": [
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.cuda.is_available(),\n",
        "      \"MONAI:\", monai.__version__)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSou67_z5kOm"
      },
      "source": [
        "## 2) Config (edit paths only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pAJaxCiL5kOm"
      },
      "outputs": [],
      "source": [
        "IMG_PATH = \"data/mri_single_egg.nii\"\n",
        "LBL_PATH = \"data/egg_label.nii\"\n",
        "\n",
        "OUT_DIR = Path(\"data/outputs\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "PP_DIR  = Path(\"data/preprocessed\"); PP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "CFG = {\n",
        "    \"labels\": {\"bg\":0, \"white\":1, \"yolk\":2, \"air\":3},\n",
        "    \"preprocess\": {\n",
        "        \"do_n4\": False,\n",
        "        \"spacing\": (1.0, 1.0, 1.0),\n",
        "        \"orientation\": \"RAS\",\n",
        "        \"percentiles\": (0.5, 99.5),\n",
        "        \"zscore_nonzero\": True\n",
        "    },\n",
        "    \"train\": {\n",
        "        # Remove these if not using patch training:\n",
        "        # \"roi\": (128,128,128),\n",
        "        # \"samples_per_iter\": 12,\n",
        "        # \"crop_ratios\": [0.2, 0.4, 0.3, 0.1],\n",
        "\n",
        "        \"batch_size\": 2,\n",
        "        \"max_iters\": 2000,        # increase for better training\n",
        "        \"lr\": 2e-4,\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"channels\": (16,32,64,128,256),\n",
        "        \"strides\":  (2,2,2,2),\n",
        "        \"include_bg_in_dice\": False,\n",
        "        \"ce_weight_cap\": (0.25, 4.0)\n",
        "    },\n",
        "    \"infer\": {\"overlap\": 0.5, \"sw_batch\": 2},  # only used with SlidingWindowInferer\n",
        "    \"seed\": 42\n",
        "}\n",
        "\n",
        "\n",
        "# deterministic-ish\n",
        "torch.manual_seed(CFG[\"seed\"]); np.random.seed(CFG[\"seed\"]); random.seed(CFG[\"seed\"])\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UalBixU5kOm"
      },
      "source": [
        "## 3) Quick data sanity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijmlHt5N5kOm",
        "outputId": "e08c5a6e-eb66-4232-96ed-44e88746fbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image shape: (36, 45, 80) Label shape: (36, 45, 80)\n",
            "Unique labels: [0 1 2 3]\n",
            "Voxel spacing from header (if present): (np.float32(1.3333334), np.float32(1.0), np.float32(1.0))\n"
          ]
        }
      ],
      "source": [
        "img_nii = nib.load(IMG_PATH); lbl_nii = nib.load(LBL_PATH)\n",
        "img = img_nii.get_fdata()\n",
        "lbl = lbl_nii.get_fdata().astype(np.int64)\n",
        "\n",
        "print(\"Image shape:\", img.shape, \"Label shape:\", lbl.shape)\n",
        "print(\"Unique labels:\", np.unique(lbl))\n",
        "print(\"Voxel spacing from header (if present):\", img_nii.header.get_zooms())\n",
        "assert img.shape == lbl.shape, \"Image/label shape mismatch.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCCaCjmu5kOm"
      },
      "source": [
        "## 4) Preprocessing (reorient → resample → normalize)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF793ll65kOm",
        "outputId": "4e4537ac-1dfb-429c-c214-b53437ac78e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed shapes: torch.Size([1, 48, 45, 80]) torch.Size([1, 48, 45, 80])\n",
            "2025-08-18 15:50:59,652 INFO image_writer.py:197 - writing: data/preprocessed/mri_single_egg_pp.nii.gz\n",
            "2025-08-18 15:50:59,703 INFO image_writer.py:197 - writing: data/preprocessed/egg_label_pp.nii.gz\n",
            "Wrote preprocessed NIfTI to data/preprocessed\n"
          ]
        }
      ],
      "source": [
        "preproc = Compose([\n",
        "    LoadImaged(keys=[\"image\", \"label\"]),\n",
        "\n",
        "    # Ensure channel-first format (C, D, H, W) instead of (D, H, W, C)\n",
        "    EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
        "\n",
        "    # Reorient to a consistent anatomical orientation (e.g., RAS)\n",
        "    Orientationd(keys=[\"image\", \"label\"], axcodes=CFG[\"preprocess\"][\"orientation\"]),\n",
        "\n",
        "    # Resample to target voxel spacing\n",
        "    #   - bilinear for continuous image data\n",
        "    #   - nearest-neighbor for discrete label masks\n",
        "    Spacingd(\n",
        "        keys=[\"image\", \"label\"],\n",
        "        pixdim=CFG[\"preprocess\"][\"spacing\"],\n",
        "        mode=(\"bilinear\", \"nearest\")\n",
        "    ),\n",
        "\n",
        "    # Intensity normalization using percentile clipping\n",
        "    # Maps voxel values between [0,1] based on given percentiles\n",
        "    ScaleIntensityRangePercentilesd(\n",
        "        keys=[\"image\"],\n",
        "        lower=CFG[\"preprocess\"][\"percentiles\"][0],\n",
        "        upper=CFG[\"preprocess\"][\"percentiles\"][1],\n",
        "        b_min=0.0, b_max=1.0, clip=True\n",
        "    ),\n",
        "\n",
        "    # Normalize intensities (per channel, non-background voxels only)\n",
        "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
        "\n",
        "    # Convert numpy arrays to torch tensors\n",
        "    ToTensord(keys=[\"image\", \"label\"])\n",
        "])\n",
        "\n",
        "# Apply transforms to input dictionary\n",
        "data_dict = {\"image\": IMG_PATH, \"label\": LBL_PATH}\n",
        "ppd = preproc(data_dict)\n",
        "\n",
        "# Extract preprocessed tensors\n",
        "img_pp = ppd[\"image\"]  # image: shape (1, D, H, W)\n",
        "lbl_pp = ppd[\"label\"].long()  # label: shape (1, D, H, W)\n",
        "\n",
        "print(\"Preprocessed shapes:\", img_pp.shape, lbl_pp.shape)\n",
        "\n",
        "# Save preprocessed NIfTI volumes (with spatial metadata preserved)\n",
        "saver = SaveImaged(\n",
        "    keys=[\"image\", \"label\"],\n",
        "    output_dir=str(PP_DIR),\n",
        "    output_postfix=\"pp\",\n",
        "    output_ext=\".nii.gz\",\n",
        "    separate_folder=False\n",
        ")\n",
        "_ = saver(ppd)\n",
        "\n",
        "print(\"Wrote preprocessed NIfTI to\", PP_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULo1C1ms5kOn"
      },
      "source": [
        "## 5) Class weights (from the mask) :\n",
        "Balancing class importance for training a segmentation model, ensuring rare structures like yolk or air are not ignored compared to abundant background voxels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TanSPkoc5kOn",
        "outputId": "3eef7f0e-1212-40b0-e511-92e1241d3b01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.25, 0.25, 0.25, 4.  ])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_ce_weights_from_label(lbl_path, labels_map, clamp=(0.25,4.0), boost_air=True):\n",
        "    lab = nib.load(lbl_path).get_fdata().astype(np.int64)\n",
        "\n",
        "    # Define class IDs\n",
        "    classes = [labels_map[\"bg\"], labels_map[\"white\"], labels_map[\"yolk\"], labels_map[\"air\"]]\n",
        "\n",
        "    # Count number of voxels belonging to each class\n",
        "    counts = np.array([(lab == c).sum() for c in classes], dtype=np.float64)\n",
        "\n",
        "    # Avoid division by zero (ensure min count = 1)\n",
        "    counts = np.maximum(counts, 1.0)\n",
        "\n",
        "\n",
        "    # Compute relative frequency of each class\n",
        "    freq = counts / counts.sum()\n",
        "\n",
        "    # Inverse frequency weighting (rare classes get higher weight)\n",
        "    w = 1.0 / freq\n",
        "\n",
        "\n",
        "    # Normalize so that mean weight = 1\n",
        "    w = w / w.mean()\n",
        "\n",
        "    # --- Adjustments ---\n",
        "\n",
        "    # De-emphasize background: making it less dominating\n",
        "    w[0] = min(w[0], 0.5 * w[1:].mean())\n",
        "\n",
        "    # Optionally boost \"air\" class (small pockets)\n",
        "    if boost_air:\n",
        "        w[3] *= 1.25\n",
        "\n",
        "    # Clamp weights within reasonable range\n",
        "    w = np.clip(w, clamp[0], clamp[1])\n",
        "\n",
        "    return w\n",
        "\n",
        "\n",
        "ce_weights = compute_ce_weights_from_label(str(PP_DIR/\"egg_label_pp.nii.gz\"), CFG[\"labels\"],\n",
        "                                           clamp=CFG[\"train\"][\"ce_weight_cap\"])\n",
        "ce_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SvCIW0L5kOn"
      },
      "source": [
        "## 6) Dataset & patch sampler (class-balanced) :\n",
        "Let's make a dataloader that loads MRI volumes,\n",
        "- standardizes their orientation/spacing/intensities,\n",
        "- apply light data augmentations\n",
        "- Prepares them as PyTorch tensors for segmentation training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6WiixQM5kOn",
        "outputId": "0ecf3833-ddc1-4a4a-8e61-dc2340417853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1, 48, 48, 80]) torch.Size([1, 1, 48, 48, 80])\n"
          ]
        }
      ],
      "source": [
        "train_transforms = Compose([\n",
        "    LoadImaged(keys=[\"image\",\"label\"]),\n",
        "\n",
        "    # Ensure channel-first format: (C, D, H, W)\n",
        "    EnsureChannelFirstd(keys=[\"image\",\"label\"]),\n",
        "\n",
        "    # Reorient to a standard anatomical orientation (e.g., RAS)\n",
        "    Orientationd(keys=[\"image\",\"label\"], axcodes=CFG[\"preprocess\"][\"orientation\"]),\n",
        "\n",
        "    # Resample to target voxel spacing\n",
        "    # - bilinear for continuous image data\n",
        "    # - nearest-neighbor for discrete label masks\n",
        "    Spacingd(\n",
        "        keys=[\"image\",\"label\"],\n",
        "        pixdim=CFG[\"preprocess\"][\"spacing\"],\n",
        "        mode=(\"bilinear\",\"nearest\")\n",
        "    ),\n",
        "\n",
        "    # Scale intensities to [0,1] using 0.5–99.5 percentile clipping\n",
        "    ScaleIntensityRangePercentilesd(\n",
        "        keys=[\"image\"],\n",
        "        lower=0.5, upper=99.5,\n",
        "        b_min=0., b_max=1.,\n",
        "        clip=True\n",
        "    ),\n",
        "\n",
        "    # Z-score normalize intensities (per channel, ignoring background)\n",
        "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
        "\n",
        "    # Pad so dimensions are divisible by 16 (useful for UNet downsampling/upsampling)\n",
        "    DivisiblePadd(keys=[\"image\",\"label\"], k=16),\n",
        "\n",
        "    # --- AUGMENTATIONS ---\n",
        "\n",
        "    # Random flips along X/Y/Z axes (each with 50% chance)\n",
        "    RandFlipd(keys=[\"image\",\"label\"], prob=0.5, spatial_axis=0),\n",
        "    RandFlipd(keys=[\"image\",\"label\"], prob=0.5, spatial_axis=1),\n",
        "    RandFlipd(keys=[\"image\",\"label\"], prob=0.5, spatial_axis=2),\n",
        "\n",
        "    # Random 90° rotations (20% chance, up to 270°)\n",
        "    RandRotate90d(keys=[\"image\",\"label\"], prob=0.2, max_k=3),\n",
        "\n",
        "    # Random affine (rotation ~±11°, scaling ±10%)\n",
        "    # Bilinear interpolation for image, nearest-neighbor for label\n",
        "    RandAffined(\n",
        "        keys=[\"image\",\"label\"],\n",
        "        prob=0.2,\n",
        "        rotate_range=(0.2,0.2,0.2),\n",
        "        scale_range=(0.1,0.1,0.1),\n",
        "        mode=(\"bilinear\",\"nearest\")\n",
        "    ),\n",
        "\n",
        "    # Add Gaussian noise to the image (15% chance)\n",
        "    RandGaussianNoised(keys=[\"image\"], prob=0.15, mean=0.0, std=0.02),\n",
        "\n",
        "    # Random contrast adjustment (gamma correction between 0.7–1.3)\n",
        "    RandAdjustContrastd(keys=[\"image\"], prob=0.15, gamma=(0.7,1.3)),\n",
        "\n",
        "    # Convert numpy arrays → torch tensors\n",
        "    ToTensord(keys=[\"image\",\"label\"])\n",
        "])\n",
        "\n",
        "\n",
        "train_ds = Dataset(\n",
        "    data=[{\"image\": str(PP_DIR/\"mri_single_egg_pp.nii.gz\"), \"label\": str(PP_DIR/\"egg_label_pp.nii.gz\")}],\n",
        "    transform=train_transforms\n",
        ")\n",
        "train_loader = DataLoader(train_ds, batch_size=CFG[\"train\"][\"batch_size\"], shuffle=True,\n",
        "                          num_workers=2, pin_memory=(device==\"cuda\"))\n",
        "\n",
        "# Peek at one batch (full volume)\n",
        "batch = next(iter(train_loader))\n",
        "print(batch[\"image\"].shape, batch[\"label\"].shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae1x5Mco5kOn"
      },
      "source": [
        "## 7) Model / loss / optimizer :\n",
        "\n",
        "Let's define the 3D U-Net model.\n",
        "\n",
        "### Loss functions :\n",
        "\n",
        "- Dice loss\n",
        "  - Measures overlap between prediction and ground truth.\n",
        "\n",
        "- Cross-entropy (CE) loss\n",
        "  - Standard classification loss. Encourages correct voxel-wise class probabilities.\n",
        "\n",
        "\n",
        "### Let's make a Combined loss (compute_loss) :\n",
        "- Uses a weighted sum: 0.7 * Dice + 0.3 * CE.\n",
        "\n",
        "- Why combine?\n",
        "  - Dice optimizes global overlap (good for segmentation quality).\n",
        "  - Cross-entropy optimizes local, voxel-level classification.\n",
        "  - We will get stable training (from CE) + better shape/overlap learning (from Dice).\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWspnA05kOn",
        "outputId": "61af3c30-18ba-4693-d95c-c7e736cc0b69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4056924812.py:41: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
          ]
        }
      ],
      "source": [
        "model = UNet(\n",
        "    spatial_dims=3,       # 3D U-Net (for volumetric data)\n",
        "    in_channels=1,        # single-channel input (e.g., grayscale MRI)\n",
        "    out_channels=4,       # number of segmentation classes (bg, white, yolk, air)\n",
        "    channels=CFG[\"train\"][\"channels\"],  # number of feature maps at each level\n",
        "    strides=CFG[\"train\"][\"strides\"],    # downsampling strides per level\n",
        "    num_res_units=2       # number of residual units per stage (adds depth)\n",
        ").to(device)              # move model to GPU/CPU\n",
        "\n",
        "dice_loss = DiceLoss(\n",
        "    include_background=CFG[\"train\"][\"include_bg_in_dice\"],  # optionally exclude bg from Dice\n",
        "    to_onehot_y=True,      # convert target labels to one-hot encoding\n",
        "    softmax=True,          # apply softmax on logits before Dice\n",
        "    squared_pred=True      # use squared predictions in denominator (stabilizes training)\n",
        ")\n",
        "\n",
        "# Cross-entropy loss: penalizes misclassifications\n",
        "# Weighted by class-balancing weights (computed earlier)\n",
        "ce_loss = torch.nn.CrossEntropyLoss(\n",
        "    weight=torch.tensor(ce_weights, dtype=torch.float32, device=device)\n",
        ")\n",
        "\n",
        "# Combined loss function (weighted sum of Dice + CE)\n",
        "def compute_loss(logits, targets):\n",
        "    # targets shape: (B, 1, D, H, W), need squeeze for CE\n",
        "    return 0.7 * dice_loss(logits, targets) + \\\n",
        "           0.3 * ce_loss(logits, targets[:,0].long())\n",
        "\n",
        "# --- OPTIMIZER ---\n",
        "# AdamW: Adam with decoupled weight decay (better regularization)\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=CFG[\"train\"][\"lr\"],                  # learning rate\n",
        "    weight_decay=CFG[\"train\"][\"weight_decay\"]  # L2 regularization\n",
        ")\n",
        "\n",
        "# --- MIXED PRECISION TRAINING (optional, only if GPU) ---\n",
        "# GradScaler: scales gradients to avoid underflow in FP16\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(device==\"cuda\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7C2SrxF5kOn"
      },
      "source": [
        "## 8) Train loop + logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bX2qXkz-5kOn",
        "outputId": "0abc1114-d6b6-4517-d9b5-73976717b943"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining Progress:   0%|          | 0/2000 [00:00<?, ?it/s]/tmp/ipython-input-3125697638.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
            "Training Progress: 100%|██████████| 2000/2000 [49:52<00:00,  1.50s/it, loss=0.0957]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: data/outputs/ckpts/unet_egg_single_overfit.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "it = 0\n",
        "loss_hist = []\n",
        "\n",
        "# Initialize progress bar\n",
        "pbar = tqdm(total=CFG[\"train\"][\"max_iters\"], desc=\"Training Progress\")\n",
        "\n",
        "# Training loop\n",
        "while it < CFG[\"train\"][\"max_iters\"]:\n",
        "    # Iterate over the training data loader\n",
        "    for batch in train_loader:\n",
        "        # Move data to the appropriate device\n",
        "        img = batch[\"image\"].to(device)            # (B,1,D,H,W)\n",
        "        y   = batch[\"label\"].long().to(device)     # (B,1,D,H,W) ints in {0..3}\n",
        "\n",
        "        # Enable automatic mixed precision (AMP) for potentially faster training\n",
        "        with torch.cuda.amp.autocast(enabled=(device==\"cuda\")):\n",
        "            # Forward pass: compute model output (logits)\n",
        "            logits = model(img)                    # (B,4,D,H,W)\n",
        "            # Compute the loss\n",
        "            loss = compute_loss(logits, y)\n",
        "\n",
        "        # Backward pass and optimizer step with gradient scaling (for AMP)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
        "\n",
        "        # Update iteration count and loss history\n",
        "        it += 1\n",
        "        loss_hist.append(float(loss.item()))\n",
        "\n",
        "        # Update progress bar every 10 iterations\n",
        "        if it % 100 == 0:\n",
        "            pbar.update(100)\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "\n",
        "        # Stop training if max iterations are reached\n",
        "        if it >= CFG[\"train\"][\"max_iters\"]:\n",
        "            break\n",
        "\n",
        "# Close the progress bar\n",
        "pbar.close()\n",
        "\n",
        "# Save the trained model checkpoint\n",
        "CKPT_PATH = OUT_DIR/\"ckpts\"; CKPT_PATH.mkdir(parents=True, exist_ok=True)\n",
        "torch.save(model.state_dict(), CKPT_PATH/\"unet_egg_single_overfit.pt\")\n",
        "print(\"Saved:\", CKPT_PATH/\"unet_egg_single_overfit.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI49i7cv5kOn"
      },
      "source": [
        "## 9) Full-volume inference (sliding window)\n",
        "Let's create a MONAI inference pipeline:\n",
        "-  Standardizes an input 3D image, run a trained 3D U-Net to get class predictions,\n",
        "- Convert logits to a discrete label volume\n",
        "- Save the result as a NIfTI while preserving correct spatial geometry (affine/metadata)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLpJxpnm5kOn",
        "outputId": "272b7335-d68e-4806-85fd-6a830aa1ebc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-18 17:07:41,858 INFO image_writer.py:197 - writing: data/outputs/preds/pred.nii.gz\n",
            "Saved raw prediction -> data/outputs/preds/pred.nii.gz\n",
            "2025-08-18 17:07:41,960 INFO image_writer.py:197 - writing: data/outputs/preds/pred_pp.nii.gz\n",
            "Saved postprocessed prediction -> data/outputs/preds/pred_pp.nii.gz\n"
          ]
        }
      ],
      "source": [
        "from monai.transforms import (\n",
        "    Compose, LoadImaged, EnsureChannelFirstd, Orientationd, Spacingd,\n",
        "    ScaleIntensityRangePercentilesd, NormalizeIntensityd, DivisiblePadd,\n",
        "    EnsureTyped\n",
        ")\n",
        "from monai.data.meta_tensor import MetaTensor\n",
        "\n",
        "infer_transforms = Compose([\n",
        "    LoadImaged(keys=[\"image\"]),\n",
        "    EnsureChannelFirstd(keys=[\"image\"]),\n",
        "    Orientationd(keys=[\"image\"], axcodes=CFG[\"preprocess\"][\"orientation\"]),\n",
        "    Spacingd(keys=[\"image\"], pixdim=CFG[\"preprocess\"][\"spacing\"], mode=(\"bilinear\",)),\n",
        "    ScaleIntensityRangePercentilesd(\n",
        "        keys=[\"image\"], lower=0.5, upper=99.5, b_min=0., b_max=1., clip=True\n",
        "    ),\n",
        "    NormalizeIntensityd(keys=[\"image\"], nonzero=True, channel_wise=True),\n",
        "    DivisiblePadd(keys=[\"image\"], k=16),\n",
        "    EnsureTyped(keys=[\"image\"], track_meta=True),  # keeps MetaTensor w/ meta + affine\n",
        "])\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Apply transforms to image file\n",
        "    batch = infer_transforms({\"image\": str(PP_DIR / \"mri_single_egg_pp.nii.gz\")})\n",
        "\n",
        "    # MetaTensor containing data + metadata\n",
        "    img: MetaTensor = batch[\"image\"]  # shape: (C, D, H, W)\n",
        "\n",
        "    # Build input tensor (B, C, D, H, W)\n",
        "    x = img.unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    logits = model(x)\n",
        "\n",
        "    # Softmax -> argmax -> numpy (D, H, W), uint8\n",
        "    pred = (\n",
        "        logits.softmax(dim=1)\n",
        "        .argmax(dim=1)\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "        .astype(np.uint8)[0]\n",
        "    )\n",
        "\n",
        "# COMMON AFFINE / META (from MetaTensor)\n",
        "\n",
        "# Prefer the MetaTensor's affine; fall back to stored meta or identity\n",
        "aff = getattr(img, \"affine\", None)\n",
        "if aff is None:\n",
        "    aff = img.meta.get(\"affine\", img.meta.get(\"original_affine\", np.eye(4)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhJ3rl4HerKM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# SAVE RAW PREDICTION\n",
        "\n",
        "out_dir = OUT_DIR / \"preds\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "writer = NibabelWriter()\n",
        "pred_np = np.expand_dims(pred, 0)  # add channel -> (C, D, H, W)\n",
        "writer.set_data_array(pred_np, channel_dim=0)\n",
        "writer.set_metadata({\n",
        "    \"affine\": aff,\n",
        "    \"original_affine\": img.meta.get(\"original_affine\", aff),\n",
        "})\n",
        "raw_path = out_dir / \"pred.nii.gz\"\n",
        "writer.write(str(raw_path), verbose=True)\n",
        "print(\"Saved raw prediction ->\", raw_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMVT4uCK5kOn"
      },
      "source": [
        "## 10) Post-processing (largest CC per class)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lab9F-Z35kOn",
        "outputId": "2857aef8-e6c6-4e76-9a0e-5346e8d0297f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-08-18 17:59:36,478 INFO image_writer.py:197 - writing: data/outputs/preds/pred_pp.nii.gz\n",
            "Saved postprocessed prediction -> data/outputs/preds/pred_pp.nii.gz\n"
          ]
        }
      ],
      "source": [
        "\n",
        "post = Compose([\n",
        "    AsDiscreted(keys=[\"pred\"], to_onehot=4),\n",
        "    KeepLargestConnectedComponentd(keys=[\"pred\"], independent=True, connectivity=3),\n",
        "    AsDiscreted(keys=[\"pred\"], argmax=True),\n",
        "])\n",
        "\n",
        "pred_pp = post({\"pred\": torch.from_numpy(pred).unsqueeze(0)})[\"pred\"]\n",
        "pred_pp = pred_pp.numpy().astype(np.uint8)[0]  # (D, H, W)\n",
        "\n",
        "writer = NibabelWriter()\n",
        "pred_pp_np = np.expand_dims(pred_pp, 0)  # (C, D, H, W)\n",
        "writer.set_data_array(pred_pp_np, channel_dim=0)\n",
        "writer.set_metadata({\n",
        "    \"affine\": aff,\n",
        "    \"original_affine\": img.meta.get(\"original_affine\", aff),\n",
        "})\n",
        "pp_path = out_dir / \"pred_pp.nii.gz\"\n",
        "writer.write(str(pp_path), verbose=True)\n",
        "print(\"Saved postprocessed prediction ->\", pp_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFtYO4X6fHH_"
      },
      "source": [
        "## Undo Preprocessing :\n",
        "Let's undo preprocessing transforms on the model’s output, so the segmentation prediction matches the original image’s orientation, spacing, and shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YlVfwPNgK3kM"
      },
      "outputs": [],
      "source": [
        "from monai.transforms import Invertd, AsDiscreted\n",
        "\n",
        "# after you compute logits -> pred_torch (C=1 or already argmaxed)\n",
        "pred_torch = logits.argmax(dim=1).cpu()  # (1, D, H, W) labels\n",
        "data = {\"image\": batch[\"image\"], \"pred\": pred_torch}\n",
        "\n",
        "post_trans = Invertd(\n",
        "    keys=\"pred\",\n",
        "    transform=infer_transforms,\n",
        "    orig_keys=\"image\",\n",
        "    meta_keys=\"pred_meta_dict\",\n",
        "    orig_meta_keys=\"image_meta_dict\",\n",
        "    meta_key_postfix=\"meta_dict\",\n",
        "    nearest_interp=True,  # for labels\n",
        "    to_tensor=True,\n",
        ")\n",
        "data = post_trans(data)\n",
        "pred_unpadded = data[\"pred\"].squeeze(0).numpy().astype(np.uint8)  # (D,H,W)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6-x8lk65kOn"
      },
      "source": [
        "## 11) Metrics (per-class Dice)\n",
        "Let's evaluate segmentation quality by computing the Dice coefficient per class (white, yolk, air), comparing the model’s predictions (pred_unpadded) against the ground truth NIfTI labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7IbvjTz5kOn",
        "outputId": "e7970995-c046-4164-91ee-05fd8a8e9f5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'white': np.float64(0.9833714721581575),\n",
              " 'yolk': np.float64(0.9808054498094609),\n",
              " 'air': np.float64(0.9941812136243211)}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gt = nib.load(PP_DIR/\"egg_label_pp.nii.gz\").get_fdata().astype(np.uint8)\n",
        "def dice_per_class(y_true, y_pred, cls):\n",
        "    t = (y_true==cls).astype(np.uint8); p = (y_pred==cls).astype(np.uint8)\n",
        "    inter = (t & p).sum()\n",
        "    denom = t.sum() + p.sum()\n",
        "    return (2*inter)/(denom+1e-8)\n",
        "\n",
        "classes = [CFG[\"labels\"][\"white\"], CFG[\"labels\"][\"yolk\"], CFG[\"labels\"][\"air\"]]\n",
        "dice_scores = { \"white\": dice_per_class(gt, pred_unpadded, CFG[\"labels\"][\"white\"]),\n",
        "                \"yolk\":  dice_per_class(gt, pred_unpadded, CFG[\"labels\"][\"yolk\"]),\n",
        "                \"air\":   dice_per_class(gt, pred_unpadded, CFG[\"labels\"][\"air\"]) }\n",
        "dice_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbJv4pyS5kOo"
      },
      "source": [
        "## 12) Visualization (overlay slices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ZBo6Dk5U5kOo",
        "outputId": "59499ff3-963f-4413-8a1b-95e49d0bd955"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_slice(vol, msk=None, z=None):\n",
        "    D = vol.shape[0]\n",
        "    z = D//2 if z is None else max(0, min(D-1, z))\n",
        "    v = vol[z]\n",
        "    plt.figure(figsize=(5,5)); plt.imshow(v, cmap=\"gray\")\n",
        "    if msk is not None:\n",
        "        # simple colors: white=blue(1), yolk=yellow(2), air=red(3)\n",
        "        overlay = np.zeros((*msk[z].shape, 3), dtype=float)\n",
        "        overlay[msk[z]==1] = [0.2,0.4,1.0]\n",
        "        overlay[msk[z]==2] = [1.0,0.9,0.2]\n",
        "        overlay[msk[z]==3] = [1.0,0.2,0.2]\n",
        "        plt.imshow(overlay, alpha=0.4)\n",
        "    plt.axis('off'); plt.show()\n",
        "\n",
        "# Load preprocessed arrays for viewing\n",
        "img_pp_np = nib.load(PP_DIR/\"mri_single_egg_pp.nii.gz\").get_fdata().astype(np.float32)\n",
        "pred_pp_np = nib.load(pp_path).get_fdata().astype(np.uint8)\n",
        "\n",
        "show_slice(img_pp_np, pred_pp_np, z=img_pp_np.shape[0]//2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWLgdFS15kOo"
      },
      "source": [
        "## 13) Export artifacts (ckpt, pred, config snapshot) :\n",
        "Let's save the trained model checkpoint, the prediction files, and a snapshot of the configuration and metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJxOR1-N5kOo",
        "outputId": "e879e946-66e7-4389-bcf3-b6d8a06320e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved config_snapshot.json and metrics.json in data/outputs\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save config + weights + metrics so the future CLI can pick them up verbatim\n",
        "(OUT_DIR / \"ckpts\").mkdir(exist_ok=True, parents=True)\n",
        "(OUT_DIR / \"preds\").mkdir(exist_ok=True, parents=True)\n",
        "with open(OUT_DIR / \"config_snapshot.json\", \"w\") as f:\n",
        "    json.dump(CFG, f, indent=2)\n",
        "\n",
        "# Add the total segmented volume to the metrics dictionary\n",
        "metrics = {\"dice\": dice_scores, \"total_segmented_volume_mm3\": total_segmented_volume_mm3}\n",
        "with open(OUT_DIR / \"metrics.json\", \"w\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(\"Saved config_snapshot.json and metrics.json in\", OUT_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9252f7b0"
      },
      "source": [
        "# Compare with the GT volume\n",
        "Calculate the volumes of the egg yolk, egg white, and air cell from the segmentation mask in \"pred_pp.nii.gz\". The ground truth total egg volume is 60 mL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ae7c6c4"
      },
      "source": [
        "## Load the postprocessed segmentation mask\n",
        "\n",
        "Load the `pred_pp.nii.gz` file which contains the segmentation of the different egg compartments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f56b55e",
        "outputId": "8cadd4bf-bd3f-4cfc-be00-e954f783b6a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded segmentation mask with shape: (48, 48, 80) and dtype: uint8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the postprocessed prediction NIfTI file\n",
        "pred_pp_nii = nib.load(\"data/outputs/preds/pred_pp.nii.gz\")\n",
        "\n",
        "# Get the image data as a NumPy array and ensure the data type is uint8\n",
        "pred_pp_np = pred_pp_nii.get_fdata().astype(np.uint8)\n",
        "\n",
        "print(\"Loaded segmentation mask with shape:\", pred_pp_np.shape, \"and dtype:\", pred_pp_np.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508757fd"
      },
      "source": [
        "## Get voxel dimensions\n",
        "\n",
        "Retrieve the voxel dimensions (spacing) from the NIfTI file header."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "025ead91",
        "outputId": "635ff152-d14d-42bd-d0ca-85be46b7e3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Voxel dimensions (x, y, z): (np.float32(1.0), np.float32(1.0), np.float32(1.0))\n"
          ]
        }
      ],
      "source": [
        "# Get the voxel dimensions (spacing) from the NIfTI header\n",
        "voxel_dims = pred_pp_nii.header.get_zooms()\n",
        "\n",
        "print(\"Voxel dimensions (x, y, z):\", voxel_dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "890d6ca5"
      },
      "source": [
        "## Calculate compartment volumes\n",
        "\n",
        "For each compartment (egg yolk, egg white, air cell), count the number of voxels assigned to that class in the segmentation mask and multiply by the volume of a single voxel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49684db7",
        "outputId": "3dbd8c13-2799-4321-cb4a-a0d7c4ff023e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Egg white volume: 9872.00 mm³\n",
            "Egg yolk volume: 25093.00 mm³\n",
            "Air cell volume: 598.00 mm³\n"
          ]
        }
      ],
      "source": [
        "# Calculate the volume of a single voxel in cubic millimeters (mm^3)\n",
        "voxel_volume_mm3 = voxel_dims[0] * voxel_dims[1] * voxel_dims[2]\n",
        "\n",
        "# Get the label values for each compartment from the configuration\n",
        "label_white = CFG[\"labels\"][\"white\"]\n",
        "label_yolk = CFG[\"labels\"][\"yolk\"]\n",
        "label_air = CFG[\"labels\"][\"air\"]\n",
        "\n",
        "# Count the number of voxels for each compartment\n",
        "white_voxel_count = np.sum(pred_pp_np == label_white)\n",
        "yolk_voxel_count = np.sum(pred_pp_np == label_yolk)\n",
        "air_voxel_count = np.sum(pred_pp_np == label_air)\n",
        "\n",
        "# Calculate the volume of each compartment in mm^3\n",
        "white_volume_mm3 = white_voxel_count * voxel_volume_mm3\n",
        "yolk_volume_mm3 = yolk_voxel_count * voxel_volume_mm3\n",
        "air_volume_mm3 = air_voxel_count * voxel_volume_mm3\n",
        "\n",
        "print(f\"Egg white volume: {white_volume_mm3:.2f} mm³\")\n",
        "print(f\"Egg yolk volume: {yolk_volume_mm3:.2f} mm³\")\n",
        "print(f\"Air cell volume: {air_volume_mm3:.2f} mm³\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4c1dd8d"
      },
      "source": [
        "## Calculate total volume from segmentation\n",
        "\n",
        "Sum the volumes of all compartments (egg white, egg yolk, and air cell) to get the total volume as calculated from the segmentation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9318395d",
        "outputId": "9e2de589-9486-409d-99a7-49464c687710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total segmented volume: 35563.00 mm³\n"
          ]
        }
      ],
      "source": [
        "# Calculate the total volume from the segmentation by summing the individual compartment volumes\n",
        "total_segmented_volume_mm3 = white_volume_mm3 + yolk_volume_mm3 + air_volume_mm3\n",
        "\n",
        "print(f\"Total segmented volume: {total_segmented_volume_mm3:.2f} mm³\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aff1210"
      },
      "source": [
        "## Compare volumes and display results\n",
        "\n",
        "Let's Compare the calculated compartment volumes and the total segmented volume with the provided ground truth total volume (60 mL) and display the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b688303c",
        "outputId": "ce1e417e-65fa-4888-baa6-314327d5422a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Volume Comparison ---\n",
            "Ground Truth Total Egg Volume: 60 mL (60000.00 mm³)\n",
            "Total Segmented Volume: 35563.00 mm³\n",
            "Difference (Segmented - Ground Truth): -24437.00 mm³\n",
            "Percentage Difference: -40.73%\n",
            "\n",
            "--- Compartment Volumes ---\n",
            "Egg white volume: 9872.00 mm³\n",
            "Egg yolk volume: 25093.00 mm³\n",
            "Air cell volume: 598.00 mm³\n"
          ]
        }
      ],
      "source": [
        "# Ground truth total egg volume in mL\n",
        "ground_truth_total_volume_mL = 60\n",
        "\n",
        "# Convert ground truth volume to mm^3 (1 mL = 1 cm^3 = 1000 mm^3)\n",
        "ground_truth_total_volume_mm3 = ground_truth_total_volume_mL * 1000\n",
        "\n",
        "print(\"--- Volume Comparison ---\")\n",
        "print(f\"Ground Truth Total Egg Volume: {ground_truth_total_volume_mL} mL ({ground_truth_total_volume_mm3:.2f} mm³)\")\n",
        "print(f\"Total Segmented Volume: {total_segmented_volume_mm3:.2f} mm³\")\n",
        "\n",
        "# Calculate the difference and percentage difference\n",
        "volume_difference_mm3 = total_segmented_volume_mm3 - ground_truth_total_volume_mm3\n",
        "percentage_difference = (volume_difference_mm3 / ground_truth_total_volume_mm3) * 100 if ground_truth_total_volume_mm3 != 0 else 0\n",
        "\n",
        "print(f\"Difference (Segmented - Ground Truth): {volume_difference_mm3:.2f} mm³\")\n",
        "print(f\"Percentage Difference: {percentage_difference:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Compartment Volumes ---\")\n",
        "print(f\"Egg white volume: {white_volume_mm3:.2f} mm³\")\n",
        "print(f\"Egg yolk volume: {yolk_volume_mm3:.2f} mm³\")\n",
        "print(f\"Air cell volume: {air_volume_mm3:.2f} mm³\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYLhxbIFVAeo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvIBMGYMgjO1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
